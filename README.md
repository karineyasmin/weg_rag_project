# âš™ï¸ WEG Motor RAG Assistant

> Sistema inteligente de consulta a manuais tÃ©cnicos usando Retrieval-Augmented Generation (RAG)

Este projeto implementa uma soluÃ§Ã£o completa de RAG desenvolvida para o desafio de Machine Learning Engineering. O sistema permite upload de manuais tÃ©cnicos em PDF e realiza consultas contextualizadas, fornecendo respostas precisas baseadas exclusivamente nos documentos indexados.

---

## ğŸ¯ VisÃ£o Geral

O **WEG Motor RAG Assistant** resolve o problema de consulta rÃ¡pida e precisa em documentaÃ§Ã£o tÃ©cnica extensa. Ao invÃ©s de buscar manualmente em PDFs, o usuÃ¡rio interage com um assistente que:

- ğŸ” **Busca semÃ¢ntica** nos documentos usando embeddings
- ğŸ¤– **Gera respostas contextualizadas** com LLMs de Ãºltima geraÃ§Ã£o
- ğŸ“š **Cita fontes** (arquivo e pÃ¡gina) para auditoria
- ğŸ›¡ï¸ **Evita alucinaÃ§Ãµes** rejeitando perguntas fora do contexto

---

## ğŸš€ Funcionalidades

### Core Features
- âœ… **Upload de Documentos**: IndexaÃ§Ã£o de mÃºltiplos arquivos PDF simultÃ¢neos
- âœ… **Processamento Inteligente**: DivisÃ£o automÃ¡tica em chunks com sobreposiÃ§Ã£o
- âœ… **Busca Vetorial**: ChromaDB com embeddings multilÃ­ngues (HuggingFace)
- âœ… **Respostas Contextualizadas**: LLMs com prompt engineering anti-alucinaÃ§Ã£o
- âœ… **CitaÃ§Ã£o de Fontes**: ReferÃªncias automÃ¡ticas (arquivo + pÃ¡gina)
- âœ… **Arquitetura Resiliente**: Fallback Gemini â†’ Ollama/Mistral

### Diferenciais
- ğŸŒ **Suporte MultilÃ­ngue**: Responde na mesma lÃ­ngua da pergunta
- ğŸ”„ **Hot-Reload**: AtualizaÃ§Ã£o de Ã­ndice sem reiniciar o sistema
- ğŸ“Š **Logs Estruturados**: Rastreamento completo de requisiÃ§Ãµes
- ğŸ³ **Deploy Simplificado**: Docker Compose com um comando

---

## ğŸ› ï¸ Stack TecnolÃ³gica

| Camada | Tecnologia | Justificativa |
|--------|-----------|---------------|
| **API** | FastAPI | Alta performance, validaÃ§Ã£o automÃ¡tica (Pydantic) |
| **OrquestraÃ§Ã£o** | LangChain | AbstraÃ§Ã£o para mÃºltiplos LLMs e integraÃ§Ãµes |
| **Vector Store** | ChromaDB | Simplicidade + persistÃªncia local |
| **Embeddings** | HuggingFace MiniLM | Modelo multilÃ­ngue eficiente |
| **LLM Principal** | Google Gemini 1.5 Flash | Baixa latÃªncia e custo |
| **LLM Fallback** | Mistral (Ollama) | ExecuÃ§Ã£o local, sem dependÃªncias externas |
| **Frontend** | Streamlit | Prototipagem rÃ¡pida de chat |
| **ContainerizaÃ§Ã£o** | Docker Compose | Isolamento e reprodutibilidade |

---

## ğŸ“¦ InstalaÃ§Ã£o e ExecuÃ§Ã£o

### PrÃ©-requisitos
- Docker `>= 20.10`
- Docker Compose `>= 2.0`
- Chave API do Google Gemini ([obtenha aqui](https://aistudio.google.com/app/apikey))

### 1ï¸âƒ£ ConfiguraÃ§Ã£o

Clone o repositÃ³rio e configure as variÃ¡veis de ambiente:

```bash
git clone https://github.com/seu-usuario/rag_project.git
cd rag_project
```

Altere o arquivo `.env` na raiz do projeto:

```env
GEMINI_API_KEY=sua_chave_api_aqui
OLLAMA_BASE_URL=http://localhost:11434
PRIMARY_MODEL=gemini-2.5-flash
FALLBACK_MODEL=mistral
```

### 2ï¸âƒ£ InicializaÃ§Ã£o

Execute todos os serviÃ§os com um Ãºnico comando:

```bash
docker-compose up --build
```

**O que acontece:**
1. Build das imagens Python customizadas
2. InicializaÃ§Ã£o do serviÃ§o Ollama
3. Download automÃ¡tico do modelo Mistral
4. Subida da API (porta 8000) e Frontend (porta 8501)

### 3ï¸âƒ£ Acesso

- **Frontend**: http://localhost:8501
- **API Docs**: http://localhost:8000/docs
- **Ollama API**: http://localhost:11434

---

## ğŸ“‘ DocumentaÃ§Ã£o da API

### `POST /documents`
**DescriÃ§Ã£o**: Indexa manuais tÃ©cnicos no sistema.

**Request**:
```bash
curl -X POST "http://localhost:8000/documents" \
  -F "files=@manual_motor.pdf" \
  -F "files=@manual_reducao.pdf"
```

**Response**:
```json
{
  "message": "Documents processed successfully",
  "documents_indexed": 2,
  "total_chunks": 347
}
```

---

### `POST /question`
**DescriÃ§Ã£o**: Realiza perguntas sobre os documentos indexados.

**Request**:
```bash
curl -X POST "http://localhost:8000/question" \
  -H "Content-Type: application/json" \
  -d '{"question": "Qual a potÃªncia nominal do motor W22?"}'
```

**Response**:
```json
{
  "answer": "A potÃªncia nominal do motor W22 varia de 0,12 a 355 kW, dependendo do modelo.",
  "references": [
    "Source: manual_w22.pdf (Page 12)",
    "Source: manual_w22.pdf (Page 34)"
  ]
}
```

---

## ğŸ’¡ Exemplos de Uso

### âœ… Perguntas TÃ©cnicas
```
"O que Ã© a PotÃªncia absorvida (Pa) de um motor?"
"Qual a fÃ³rmula para cÃ¡lculo de torque mencionada no manual?"
"Quais os requisitos para instalaÃ§Ã£o em ambiente explosivo?"
```

### âœ… Perguntas em InglÃªs
```
"What is the motor's power consumption?"
"How to verify insulation resistance?"
```

### âŒ Teste Anti-AlucinaÃ§Ã£o
```
Pergunta: "Qual a previsÃ£o do tempo para amanhÃ£?"
Resposta: "Information not found."
```
*(O sistema rejeita perguntas fora do contexto dos documentos)*

---

## ğŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Streamlit  â”‚â”€â”€â”€â”€â”€â–¶â”‚   FastAPI   â”‚â”€â”€â”€â”€â”€â–¶â”‚  ChromaDB   â”‚
â”‚  Frontend   â”‚      â”‚     API     â”‚      â”‚ Vector Storeâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚  LLM Manager â”‚
                     â”‚              â”‚
                     â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                     â”‚ â”‚  Gemini  â”‚ â”‚ (Primary)
                     â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                     â”‚      â–¼       â”‚
                     â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                     â”‚ â”‚  Mistral â”‚ â”‚ (Fallback)
                     â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Fluxo de Processamento
1. **IngestÃ£o**: PDF â†’ PyPDF â†’ RecursiveTextSplitter â†’ Embeddings â†’ ChromaDB
2. **Consulta**: Pergunta â†’ Busca SemÃ¢ntica (top-k=3) â†’ Prompt Engineering â†’ LLM â†’ Resposta

---

## ğŸ§ª Testes

### Teste Manual (via cURL)
```bash
# 1. Indexar documento
curl -X POST "http://localhost:8000/documents" \
  -F "files=@data/manual_teste.pdf"

# 2. Fazer pergunta
curl -X POST "http://localhost:8000/question" \
  -H "Content-Type: application/json" \
  -d '{"question": "Qual a tensÃ£o nominal?"}'
```

### Logs de DepuraÃ§Ã£o
```bash
docker-compose logs -f api
```

---

## ğŸ”§ ConfiguraÃ§Ãµes AvanÃ§adas

### Ajustar Tamanho dos Chunks
Edite [`app/services/ingestion.py`](app/services/ingestion.py):
```python
self.splitter = RecursiveCharacterTextSplitter(
    chunk_size=1500,      # Aumentar para chunks maiores
    chunk_overlap=300     # Aumentar sobreposiÃ§Ã£o
)
```

### Trocar Modelo de Embeddings
Edite [`app/providers/vector_store.py`](app/providers/vector_store.py):
```python
self.embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"  # Modelo alternativo
)
```

### Usar Apenas Ollama (sem Gemini)
No [`.env`](.env):
```env
GEMINI_API_KEY=""  # Deixar vazio forÃ§a fallback
```

---

## ğŸ“‚ Estrutura do Projeto

```
rag_project/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/              # Rotas FastAPI
â”‚   â”œâ”€â”€ config/           # VariÃ¡veis de ambiente
â”‚   â”œâ”€â”€ models/           # Schemas Pydantic
â”‚   â”œâ”€â”€ providers/        # IntegraÃ§Ãµes (LLM, Vector Store)
â”‚   â”œâ”€â”€ services/         # LÃ³gica de negÃ³cio
â”‚   â””â”€â”€ utils/            # Logging
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ vector_store/     # Banco de vetores persistido
â”‚   â””â”€â”€ temp_uploads/     # PDFs temporÃ¡rios
â”œâ”€â”€ app_frontend.py       # Interface Streamlit
â”œâ”€â”€ docker-compose.yml    # OrquestraÃ§Ã£o de containers
â”œâ”€â”€ Dockerfile            # Imagem Python customizada
â””â”€â”€ pyproject.toml        # DependÃªncias do projeto
```

---

## ğŸ› Troubleshooting

### Erro: `Ollama connection refused`
**SoluÃ§Ã£o**: Aguarde ~30s para o modelo Mistral ser baixado:
```bash
docker-compose logs ollama-pull-model
```

### Erro: `Gemini API key invalid`
**SoluÃ§Ã£o**: Verifique se a chave estÃ¡ correta no [`.env`](.env) e reinicie:
```bash
docker-compose down
docker-compose up --build
```

### Embeddings lentos na primeira execuÃ§Ã£o
**SoluÃ§Ã£o**: O modelo HuggingFace Ã© baixado no primeiro uso (~400MB). Aguarde o download.

---

## ğŸ“„ LicenÃ§a

Este projeto foi desenvolvido como parte de um desafio tÃ©cnico e estÃ¡ disponÃ­vel sob a licenÃ§a MIT.

---

## ğŸ‘¤ Autor

**Karine**  
ğŸ“§ Email: [karine.y.ribeiro@gmail.com](mailto:karine.y.ribeiro@gmail.com)  
ğŸ”— LinkedIn: [Karine Yasmin Ribeiro](https://linkedin.com/in/karine-yasmin)

---

**Desenvolvido com â¤ï¸ usando Python e LangChain**